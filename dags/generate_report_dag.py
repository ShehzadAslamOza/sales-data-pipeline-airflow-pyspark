from airflow.decorators import dag, task
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.models import Variable
from datetime import datetime
import boto3
import os
@dag(
    start_date=datetime(2024, 12, 31),
    schedule_interval="0 14 * * *", 
    catchup=False,
    tags=['spark', 'report'],
)
def generate_report_dag():
    """
    Airflow DAG to submit a Spark job, rename the output file, move it to a final S3 location,
    delete intermediate files, and save a local copy.
    """

    """
    CONFIG Dictionary:
    - PYSPARK_SCRIPT: Path to the Python script to be executed by Spark.
    - AWS_ACCESS_KEY_ID: AWS access key for authentication (from Airflow Variables).
    - AWS_SECRET_ACCESS_KEY: AWS secret key for authentication (from Airflow Variables).
    - SPARK_MASTER_URL: Spark cluster's master node URL.
    - LOCAL_CSV_PATH: Path to the local CSV input file.
    - S3_INPUT_PATH: S3 path where the raw input file is stored.
    - S3_OUTPUT_PATH: Intermediate S3 location where the Spark job writes its output.
    - FINAL_OUTPUT_PATH: Final S3 destination for the renamed and processed file.
    - LOCAL_SAVE_PATH: Local directory where the final file will be saved.
    - BUCKET_NAME: Name of the S3 bucket where the data resides.
    - GENERATED_PREFIX: S3 prefix for intermediate files generated by the Spark job.
    - FINAL_PREFIX: S3 prefix for the final renamed file.
    """
    CONFIG = {
        "PYSPARK_SCRIPT": "include/scripts/generate_report.py",
        "AWS_ACCESS_KEY_ID": Variable.get("AWS_ACCESS_KEY_ID"), 
        "AWS_SECRET_ACCESS_KEY": Variable.get("AWS_SECRET_ACCESS_KEY"), 
        "SPARK_MASTER_URL": "spark://spark-master:7077",
        "LOCAL_CSV_PATH": "include/input/OnlineRetail.csv",
        "S3_INPUT_PATH": "s3a://manager-store-report-3342/input/OnlineRetail.csv",
        "S3_OUTPUT_PATH": "s3a://manager-store-report-3342/output",
        "FINAL_OUTPUT_PATH": "manager-store-report-3342/output/reports",
        "LOCAL_SAVE_PATH": "include/reports",
        "BUCKET_NAME": "manager-store-report-3342",
        "GENERATED_PREFIX": "output/spark_output/",
        "FINAL_PREFIX": "output/reports/",
    }

    # Ensure the local save directory exists
    os.makedirs(CONFIG["LOCAL_SAVE_PATH"], exist_ok=True)

    # Submit Spark job
    submit_spark_job = SparkSubmitOperator(
        task_id="submit_spark_job",
        conn_id="my_spark_conn",
        application=CONFIG["PYSPARK_SCRIPT"],
        application_args=[
            CONFIG["LOCAL_CSV_PATH"],
            CONFIG["S3_INPUT_PATH"],
            CONFIG["S3_OUTPUT_PATH"],
            CONFIG["SPARK_MASTER_URL"],
        ],
        verbose=True,
        name="generate_report",
        packages="org.apache.hadoop:hadoop-aws:3.3.4,com.amazonaws:aws-java-sdk-bundle:1.12.262",
        conf={
            "spark.hadoop.fs.s3a.access.key": CONFIG["AWS_ACCESS_KEY_ID"],
            "spark.hadoop.fs.s3a.secret.key": CONFIG["AWS_SECRET_ACCESS_KEY"],
            "spark.hadoop.fs.s3a.endpoint": "s3.amazonaws.com",
            "spark.hadoop.fs.s3a.path.style.access": "true",
            "spark.sql.legacy.timeParserPolicy": "LEGACY",
        },
        env_vars={
            "AWS_ACCESS_KEY_ID": CONFIG["AWS_ACCESS_KEY_ID"],
            "AWS_SECRET_ACCESS_KEY": CONFIG["AWS_SECRET_ACCESS_KEY"],
        },
    )

    @task
    def process_s3_output():
        """
        Rename the part file, move it to the final S3 location, save a local copy, and clean up intermediate files.
        """
        # Initialize boto3 S3 client
        s3_client = boto3.client(
            's3',
            aws_access_key_id=CONFIG["AWS_ACCESS_KEY_ID"],
            aws_secret_access_key=CONFIG["AWS_SECRET_ACCESS_KEY"],
        )

        # Locate the part file in the generated folder
        response = s3_client.list_objects_v2(
            Bucket=CONFIG["BUCKET_NAME"], Prefix=CONFIG["GENERATED_PREFIX"]
        )
        part_file = next(
            (obj['Key'] for obj in response.get('Contents', []) if 'part-' in obj['Key']),
            None,
        )
        if not part_file:
            raise FileNotFoundError("No part file found in the generated output directory.")

        # Create a timestamped file name
        timestamp = datetime.now().strftime("%Y-%m-%d-%H-%M-%S")
        new_file_name = f"summary_{timestamp}.csv"
        new_s3_key = f"{CONFIG['FINAL_PREFIX']}{new_file_name}"

        # Rename and move the file to the final location
        s3_client.copy_object(
            Bucket=CONFIG["BUCKET_NAME"],
            CopySource={'Bucket': CONFIG["BUCKET_NAME"], 'Key': part_file},
            Key=new_s3_key,
        )
        print(f"File renamed and moved to: s3://{CONFIG['BUCKET_NAME']}/{new_s3_key}")

        # Download the renamed file locally
        local_file_path = os.path.join(CONFIG["LOCAL_SAVE_PATH"], new_file_name)
        s3_client.download_file(CONFIG["BUCKET_NAME"], new_s3_key, local_file_path)
        print(f"File saved locally at: {local_file_path}")

        # Clean up intermediate files
        for obj in response.get('Contents', []):
            s3_client.delete_object(Bucket=CONFIG["BUCKET_NAME"], Key=obj['Key'])
        print(f"Deleted intermediate folder: s3://{CONFIG['BUCKET_NAME']}/{CONFIG['GENERATED_PREFIX']}")

    # Define task dependencies
    submit_spark_job >> process_s3_output()


# Instantiate the DAG
generate_report_dag()

